\documentclass[11pt,a4paper,twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage[justification=centering]{caption}
\usepackage{subcaption}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\author{Daniel Deutsch and Dan Crankshaw}
\title{Machine Learning: Final Paper}
\date{}
\begin{document}
\maketitle

\section{Introduction}

A common problem in the field of machine learning is how do you create a classification algorithm that can read handwritten words. The applications of such a model are clear, and have already been implemented in situations like the US postal service to sort mail based on the zip code. 

The idea behind our approach is to compare how well two algorithms perform in completing the optical character recognition task. The first algorithm is an artifical neural network that will try to identify the characters based on their pixel values. The second algorithm is a combination of the preivous neural network and a hidden markov model, which will take into account the probability of specific letter combinations. We hope that addition of knowledge about the surrounding letters will help improve the accuracy of the neural network.

\section{Related Work}

Feng et al.\cite{feng2008hidden} explores this idea of combining classification with a sequence model in order to perform handwriting recognition on a large vocabulary corpus. They used a joint-boosting algorithm to not only identify the letters, but perform letter segemenation. They combined information about each individual letter with knowledge of the context of the letter in the form of a supervised HMM.

Although the approach they take is more complicated, including the use of an emsemble of HMMs and many more features, their success indicates that the combination of classification with an HMM is a viable approach for the handwriting recognition problem.



\section{The Algorithms and Data}

\subsection*{Data}

The data that will be used in the training and evaluation of the ANN and the HMM was gathered from
\href{http://ai.stanford.edu/~btaskar/ocr/}{an optical character recognition dataset from Stanford University}. The data is a collection of handwritten letters segemented from words in which all uppercase characters have been removed. We took the raw data and removed specific features included in the data that we intended not to use. The resulting data set is a list of binary pixel values of each character's 16 by 8 image.
\begin{figure}[h]
\centering
\includegraphics{img/ml.jpg}
\includegraphics{img/alphabet.jpg}
\caption{An example of the data set}
\end{figure}

The data can often be difficult to read for a human. Figure 1 demonstrates this fact. Although it is possible to make out each letter, this task is made much easier by looking at the context of the word and anticipating what the letter should be. The figure also illustrates that many of the same letters are a variety of different sizes and are placed at different locations within the image. The quality of the data may make it difficult for the ANN to learn how to recognize each letter. However, the combination of the ANN and the HMM may be a good simulation of how a human would approach the problem: try to recognize the letter, then use context clues to figure out which character it is.

\subsection*{The Algorithms}

The two algorithms that we intend to use to accomplish the character recognition goal are an artificial neural network (ANN) and a hidden markov model (HMM). The idea is to compare the accuracy of the ANN to a combination of both the ANN and HMM to see if adding knowledge of word context to the model increases accuracy on the datasets.

The ANN will be trained on the pixel values for each letter image. Each training example is a 16 by 8 pixel image which amounts to an input size of 128. In order to help improve the accuracy of the classifier, we included a bias term on the input and hidden layers. Therefore, the input size for the ANN is 129 nodes.

Similarly, since we are trying to classify each image as a letter, there will need to be 26 output nodes, one for each lowercase letter in the alphabet. Therefore, the input and output sizes are set. The only parameters left to tune for the ANN are the number of hidden layers and the number of nodes within each hidden layer.

In order to find the best values for these parameters, we ran a series of tests on the development data set to try and tune the parameters. We compared the accuracies of each model on the development dataset to try and find the optimal parameters.

The following was generated by randomly initializing an ANN with the given hidden layer sizes, then training them all on the same data. This process is repeated 20 times per data point, and the average accuracy is given. The accuracy is determined by if the classifier gets the data point exactly correct. Another common metric is to test if it got the right answer within the top 5 guesses, but that is not used here.

For the ANN with one hidden layer, the only parameter than can be changed is the number of hidden nodes in that layer.
\begin{figure}[h]
\caption{Accuracies of two-layer networks}
\centering
\begin{tabular}{|c|c|}
\hline 
Hidden Nodes & Accuracy \\ 
\hline 
25 & 54.86\% \\ 
\hline 
50 & 54.24\% \\ 
\hline 
75 & 52.13\% \\ 
\hline 
\end{tabular} 
\end{figure}

Since all three data points are relatively similar, and the smaller neural network will train faster, we will a network with one hidden layer and 25 nodes in that layer.

This data is indicative of our belief that the neural network might have some difficultly classifying this dataset. The input seems to be a relatively difficult dataset with some noise. Since adding more nodes to the hidden layer did not imporve the accuracy significantly enough, it's evident that the network is having a hard time learning the data. It may have reached its maximal ability to learn. We hope that the combination of the HMM with the ANN will help improve these accuracies.

One other idea that we tried was to add a second hidden layer to see if increasing the number of weights allowed for improvement. With a small number of tests, the amount of improvement was almost non-existent. Therefore, given the extra amount of time it takes to train a neural network with two hidden layers, we concluded that it was not worth the investment.

Therefore, the neural network that we will use in our experiments in combination with and against the HMM will be a neural network with 128 input nodes, 25 hidden nodes, and 26 output nodes, not counting the bias units.


\section{Results}

In order to compare the implementation and accuracy of the ANN versus the HMM-ANN combination, we compared their performances with three different metrics: the percentage of letters correct, the percentage of fully correct words, and the average percentage of letters correct per word. 

In the following figures, we compare the two algorithms with each of the three accuracy metrics. The neural network was not retrained between tests. Its results were used to calculate the emission probabilities for the HMM. The HMM, however, was retrained with four different subsets of the training corpus: the first 12.5\% of the corpus, the first 25\%, the first 50\%, and then all of the training data. Each algorithm was then tested on a random 10\% of the same test dataset, and the results were averaged. This process was repeated on three subsets of words: small (2-4 letters), medium (5-8 letters), and large (9+ letters) words.

\begin{figure}[h]
\centering
\caption{Percentage of letters correct}
\includegraphics[scale=0.55]{img/lettersCorrect.png}
\end{figure}

\begin{figure}[h]
\centering
\caption{Percentage of fully correct words}
\includegraphics[scale=0.55]{img/wordsCorrect.png}
\end{figure}

\begin{figure}[h]
\centering
\caption{Average percentage of letters correct per word}
\includegraphics[scale=0.55]{img/wordCorrectness.png}
\end{figure}


\section{Analysis}

We hypothesized that increasing the size of the training corpus would increase the accuracy of the HMM because in order for the transmission probabilities to converge, we would need to look at a certain amount of data. However, there was no significant difference in the HMM accuracy as we changed the size of the training corpus. It is possible that even the smallest corpus was already large enough for the transmission probabilites to have converged at approximately 14,000 words.

From Figures 3 and 5, it is evident that as the length of the word increases, the accuracy of the ANN-HMM combination decreases. We hypothesize that once the HMM begins to incorrectly identify a sequence, it tends to continue to do so because of the weight of the transmission probabilities on predicting the most probable sequence. In order to get a sequence back on the right track, we would need a high correct emission probability and a high probability of transitioning from the previous incorrect letter to the current correct letter, which effectively breaks the reasons behind using a HMM. Because we use the same ANN-HMM combination on our short and long word predictions, we assume the same probability of getting a starting down an incorrect subsequence at a given position in a word regardless of the length of the word. Because the longer words have more positions at which to start down a wrong sequence, and will tend to have more letters remaining in the word to be wrongly predicted once an incorrect subsequence is started down, longer words will tend to have more incorrect letters. *****However, this is still a hypothesis and we would need to run more experiments to verify this.

One interesting trend that we did not expect to see in the data is the slight increase in accuracy of the ANN on the data set as the word length increases. This was surprising because the ANN predicts each letter independently, so changing how the letters are grouped into words should not have affected the accuracy. One potential explanation is that the ANN is much better at classifying some letters than others. If the longer words tend to contain higher concentrations of those ``eaiser'' letters, then the ANN would have a higher accuracy on the larger word bucket. 

In order to investigate this further, we looked at the accuracy of the ANN on each specific letter. As you can see from Figure 6, the results are highly variable, and there are even several letters that have 0\% accuracy.

\begin{figure}[h]
\centering
\caption{Individual letter accuracies for the ANN}
\includegraphics[scale=0.55]{img/letterPercentages.png}
\end{figure} 

One potential reason for this high variability is that the number of training examples for the different letters varies by an order of magnitude. The letters on which we did predicted very poorly tend to occur less often in English, and therefore we did not have as many training examples for those letters. This does not prove that the longer words contain less of the harder letters, but it supports that it is a valid hypothesis. 


\section{Conclusion}

While our models did significantly better than random guessing, we still expected higher accuraices that what we were able to obtain. It seems that the root of our issues with the HMM are directly tied to the low accuracies of the neural network. We believe a lot of the issues with the neural network were based on what turned out to be a very noisy and relatively sparse data set. 

One step in trying to improve the model would be to use less noisy data. We could perform optical character recognition on typed letters instead of handwritten ones as this would minimize the amount of variation between each letter. 

Another option would be to add more features to the data set other than just the character's pixel values. We could consult the computer vision literature to help with this additional feature engineering. 


\bibliography{refs.bib}
\bibliographystyle{plain}

\end{document}